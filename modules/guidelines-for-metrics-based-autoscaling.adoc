:_module-type: CONCEPT

[id="guidelines-for-metrics-based-autoscaling_{context}"]

= Guidelines for metrics-based autoscaling
[role="_abstract"]

You can use metrics-based autoscaling to scale your AI workloads based on latency or throughput-focused Service Level Objectives (SLOs) as opposed to traditional request concurrency. Metrics-based autoscaling is based on Kubernetes Event-driven Autoscaling (KEDA).

Traditional scaling methods, which depend on factors like request concurrency, request rate, or CPU utilization, are not effective for scaling LLM inference servers that operate on GPUs. In contrast, vLLM capacity is determined by the size of the GPU and the total number of tokens processed simultaneously. Custom metrics facilitate autoscaling decisions that align directly with the desired service level objective (SLO).

The following sections details best practices for autoscaling AI inference workloads, including selecting metrics, defining sliding windows, configuring HPA scale-down settings, and taking model size into account for optimal scaling performance.

== Choosing metrics for latency and throughput-optimized scaling

For latency-sensitive applications, the choice of scaling metrics depends heavily on the characteristics of the requests:

* For use cases with a high degree of variance in sequence length, it is advisable to use SLOs defined by `Time to First Token` (TTFT) and `Inter-Token Latency` (ITL). These metrics provide more stable scaling signals that are less affected by natural fluctuations in input and output sequence lengths.

* For use cases where requests are of similar sequence lengths, it is recommended to use the `end-to-end request latency` as a scaling trigger, as the variance in input and output sequence length is minimal.
//End-to-end (e2e) request latency depends on sequence length, posing challenges for use cases with high variance in input/output token counts. A 10 token completion and a 2000 token completion will have vastly different latencies even under identical system conditions.

To maximize throughput without latency constraints, you can scale your workloads by using the `vllm:num_requests_waiting > 0.1` metric. This ensures that any queued requests immediately trigger scale-up, maximizing system utilization while preventing request backlog accumulation. This strategy works best with homogeneous input and output sequence lengths.

The specifications below outline best practices for building effective metrics-based autoscaling, including choosing the right metrics, progressively tuning configurations, and validating behavior through rigorous testing.

* *Metric selection strategy*
** Analyze your load patterns to determine sequence length variance.
** Choose TTFT/ITL for high-variance workloads, and e2e latency for uniform workloads.
** Implement multiple metrics with different priorities for robust scaling decisions.

* *Progressive optimization*
** Start with conservative thresholds and longer windows.
** Monitor scaling behavior and SLO compliance over time.
** Gradually optimize based on observed patterns and business requirements.

* *Testing and validation*
** Implement comprehensive load testing with realistic sequence length distributions.
** Validate scaling behavior under various traffic patterns.
** Test edge cases such as traffic spikes and gradual load increases.

== Choosing the right sliding window

In the context of autoscaling, the sliding window length is the time period over which metrics are aggregated or evaluated to make scaling decisions. The sliding window length heavily impacts scaling responsiveness and stability.

* *Short windows (< 30 seconds)*:
    ** Do not effectively trigger autoscaling for metric scraping intervals that fall within this window.
* *Medium windows (60 seconds)*:
    ** Offer rapid response to load changes and better handling of sudden spikes.
    ** Potential for scaling thrashing and higher infrastructure costs due to over-provisioning.
    ** Best for loads with sharp, unpredictable spikes.
*  *Longer windows (> 4 minutes)*:
    ** Provide a balance between responsiveness and stability, reducing scaling noise.
    ** May overlook brief spikes in requests and adapt more slowly to ongoing changes to sustained load.
    ** Best for production workloads with moderate load variability.
*   *Window length impact on metric types*:
    **  **TTFT/ITL metrics:** Can use shorter windows (1-2 minutes) since they are less noisy than `e2e latency`.
    **  **e2e latency metrics:** Require longer windows (4-5 minutes) to smooth out sequence length variance.

== Optimizing HPA scale-down configuration

Effective scale-down configuration is crucial for cost optimization and resource efficiency. It requires balancing the need to quickly terminate idle pods to reduce cluster load with the consideration of maintaining them to avoid cold startup times if demand returns soon. The Horizontal Pod Autoscaler (HPA) configuration for scale-down plays a critical role in removing idle pods promptly and preventing unnecessary resource usage.

You can control how the Horizontal Pod Autoscaler (HPA) scales down your application by managing the KEDA `scaledObject` resource. This resource is a Custom Resource (CR) that enables event-driven autoscaling for a specific workload.

To configure the scale-down behavior of the HPA, you must adjust the `advanced.horizontalPodAutoscalerConfig.behavior.scaleDown.stabilizationWindowSeconds` field within the `scaledObject` CR as shown in the example:

[source, YAML]
----
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: my-app-scaler
spec:
  advanced:
    horizontalPodAutoscalerConfig:
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 300

----
The optimal configuration depends on finding the right balance between resource efficiency and application responsiveness for your particular workload requirements.

== Considering model size for optimal scaling

When implementing metrics-based autoscaling for AI inference workloads, you must consider the impact of model size on scaling behavior and resource allocation strategies. The specifications below outline default configurations and typical performance characteristics to guide your approach.

* **Small models (< 3B parameters):**
    ** Memory footprint: < 6 GiB.
    ** Suitable for aggressive scaling policies with lower resource buffers.
    ** Cold start times: Up to 10 minutes to download and 30 seconds to load.
* **Medium models (3B-10B parameters):**
    ** Memory footprint: 6-20 GiB.
    ** Require more conservative scaling due to the size of the model.
    ** Cold start times: Up to 30 minutes to download and 1 minute to load.
* **Large models (> 10B parameters):**
    **  Memory footprint: > 20 GiB.
    **  May require model sharding or quantization for efficient scaling.
    **  Cold start times: Up to hours to download and several minutes to load.

For models with fewer than 3 billion parameters, you can use various techniques to significantly reduce cold start latency:

* **Container image optimization:**
    ** Prebuild images with models embedded rather than downloading models at runtime.
    ** Use multi-stage builds to minimize the final image size.
    ** Leverage image layer caching for faster container pulls.
* **Model caching strategies:**
    ** Define Persistent Volume Claims (PVCs) for shared model storage across replicas.
    ** Configure your inference service to use the PVC.


[role="_additional-resources"]
.Additional resources
* https://docs.vllm.ai/en/latest/serving/distributed_serving.html[Distributed serving]
